{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/dcaled/convert_bert_to_long/blob/main/convert_bert_to_long.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aad_1s7ybD5o"
   },
   "source": [
    "# `BERT` --> `Longformer`: build a \"long\" version of pretrained models\n",
    "\n",
    "This notebook replicates the procedure descriped in the [Longformer paper](https://arxiv.org/abs/2004.05150) to train a Longformer model starting from the BERT checkpoint. The same procedure can be applied to build the \"long\" version of other pretrained models as well. It was inspired by the notebook provided by Allenai to convert RoBERTa to Longformer: [convert_model_to_long.ipynb](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BieXv0YUd7NF"
   },
   "source": [
    "### Libraries, and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o3yjIYKXw3rL",
    "outputId": "c1290c72-07d6-4060-b14c-51073d59701f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0.2\n",
      "  Downloading transformers-3.0.2-py3-none-any.whl (769 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m769.0/769.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/tomasg/anaconda3/lib/python3.8/site-packages (from transformers==3.0.2) (3.9.0)\n",
      "Requirement already satisfied: numpy in /home/tomasg/anaconda3/lib/python3.8/site-packages (from transformers==3.0.2) (1.21.5)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92 in /home/tomasg/anaconda3/lib/python3.8/site-packages (from transformers==3.0.2) (0.1.91)\n",
      "Requirement already satisfied: requests in /home/tomasg/anaconda3/lib/python3.8/site-packages (from transformers==3.0.2) (2.25.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/tomasg/anaconda3/lib/python3.8/site-packages (from transformers==3.0.2) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/tomasg/anaconda3/lib/python3.8/site-packages (from transformers==3.0.2) (2022.7.9)\n",
      "Requirement already satisfied: packaging in /home/tomasg/anaconda3/lib/python3.8/site-packages (from transformers==3.0.2) (22.0)\n",
      "Collecting tokenizers==0.8.1.rc1\n",
      "  Downloading tokenizers-0.8.1rc1-cp38-cp38-manylinux1_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: sacremoses in /home/tomasg/anaconda3/lib/python3.8/site-packages (from transformers==3.0.2) (0.0.43)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/tomasg/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.0.2) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/tomasg/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.0.2) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/tomasg/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.0.2) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/tomasg/anaconda3/lib/python3.8/site-packages (from requests->transformers==3.0.2) (1.26.13)\n",
      "Requirement already satisfied: six in /home/tomasg/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==3.0.2) (1.16.0)\n",
      "Requirement already satisfied: joblib in /home/tomasg/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==3.0.2) (1.1.1)\n",
      "Requirement already satisfied: click in /home/tomasg/anaconda3/lib/python3.8/site-packages (from sacremoses->transformers==3.0.2) (8.0.4)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.8.1rc2\n",
      "    Uninstalling tokenizers-0.8.1rc2:\n",
      "      Successfully uninstalled tokenizers-0.8.1rc2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 3.3.1\n",
      "    Uninstalling transformers-3.3.1:\n",
      "      Successfully uninstalled transformers-3.3.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "simalign 0.2 requires networkx==2.4, but you have networkx 2.8.4 which is incompatible.\n",
      "sentence-transformers 1.1.0 requires transformers<5.0.0,>=3.1.0, but you have transformers 3.0.2 which is incompatible.\n",
      "easse 0.2.4 requires nltk==3.4.3, but you have nltk 3.7 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.8.1rc1 transformers-3.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==3.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "U0NnMMl6wy7Q"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import math\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from dataclasses import dataclass, field\n",
    "from transformers import AutoModel, AutoTokenizer, BertTokenizerFast, BertModel, BertForSequenceClassification #BertForMaskedLM\n",
    "from transformers import TrainingArguments, HfArgumentParser\n",
    "from transformers.modeling_longformer import LongformerSelfAttention\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xgoNVJYUbD59"
   },
   "source": [
    "### BertLong\n",
    "\n",
    "`BertLongForMaskedLM` represents the \"long\" version of the `BERT` model. It replaces `BertSelfAttention` with `BertLongSelfAttention`, which is a thin wrapper around `LongformerSelfAttention`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "J9EBISkRxPjO"
   },
   "outputs": [],
   "source": [
    "class BertLongSelfAttention(LongformerSelfAttention):\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        return super().forward(hidden_states, attention_mask=attention_mask, output_attentions=output_attentions)\n",
    "\n",
    "\n",
    "class BertLong(BertModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        for i, layer in enumerate(self.encoder.layer):\n",
    "            # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "            layer.attention.self = BertLongSelfAttention(config, layer_id=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LRZa5s1bD6E"
   },
   "source": [
    "Starting from the `bert-base` checkpoint, the following function converts it into an instance of `BertLong`. It makes the following changes:\n",
    "\n",
    "- extend the position embeddings from `512` positions to `max_pos`. In Longformer, we set `max_pos=4096`\n",
    "\n",
    "- initialize the additional position embeddings by copying the embeddings of the first `512` positions. This initialization is crucial for the model performance (check table 6 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) for performance without this initialization)\n",
    "\n",
    "- replaces `modeling_bert.BertSelfAttention` objects with `modeling_longformer.LongformerSelfAttention` with a attention window size `attention_window`\n",
    "\n",
    "The output of this function works for long documents even without pretraining. Check tables 6 and 11 in [the paper](https://arxiv.org/pdf/2004.05150.pdf) to get a sense of the expected performance of this model before pretraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-m4A_ttixPuf"
   },
   "outputs": [],
   "source": [
    "def create_long_model(save_model_to, attention_window, max_pos):\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', model_max_length=max_pos)\n",
    "    config = model.config\n",
    "\n",
    "    print(max_pos)\n",
    "    # extend position embeddings\n",
    "    tokenizer.model_max_length = max_pos\n",
    "    tokenizer.init_kwargs['model_max_length'] = max_pos\n",
    "    current_max_pos, embed_size = model.embeddings.position_embeddings.weight.shape\n",
    "    config.max_position_embeddings = max_pos\n",
    "    assert max_pos > current_max_pos\n",
    "    # allocate a larger position embedding matrix\n",
    "    new_pos_embed = model.embeddings.position_embeddings.weight.new_empty(max_pos, embed_size)\n",
    "    print(new_pos_embed.shape)\n",
    "    print(model.embeddings.position_embeddings)\n",
    "    # copy position embeddings over and over to initialize the new position embeddings\n",
    "    k = 0\n",
    "    step = current_max_pos\n",
    "    while k < max_pos - 1:\n",
    "        new_pos_embed[k:(k + step)] = model.embeddings.position_embeddings.weight\n",
    "        k += step\n",
    "    print(new_pos_embed.shape)\n",
    "    model.embeddings.position_ids = torch.from_numpy(tf.range(new_pos_embed.shape[0], dtype=tf.int32).numpy()[tf.newaxis, :])\n",
    "    model.embeddings.position_embeddings = torch.nn.Embedding.from_pretrained(new_pos_embed)\n",
    "    \n",
    "    # replace the `modeling_bert.BertSelfAttention` object with `LongformerSelfAttention`\n",
    "    config.attention_window = [attention_window] * config.num_hidden_layers\n",
    "    for i, layer in enumerate(model.encoder.layer):\n",
    "        longformer_self_attn = LongformerSelfAttention(config, layer_id=i)\n",
    "        longformer_self_attn.query = layer.attention.self.query\n",
    "        longformer_self_attn.key = layer.attention.self.key\n",
    "        longformer_self_attn.value = layer.attention.self.value\n",
    "\n",
    "        longformer_self_attn.query_global = layer.attention.self.query\n",
    "        longformer_self_attn.key_global = layer.attention.self.key\n",
    "        longformer_self_attn.value_global = layer.attention.self.value\n",
    "\n",
    "        layer.attention.self = longformer_self_attn\n",
    "    print(model.embeddings.position_ids.shape)\n",
    "    logger.info(f'saving model to {save_model_to}')\n",
    "    model.save_pretrained(save_model_to)\n",
    "    tokenizer.save_pretrained(save_model_to)\n",
    "    return model, tokenizer, new_pos_embed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqY_Hg5HbD6a"
   },
   "source": [
    "**Training hyperparameters**\n",
    "\n",
    "- Following BERT pretraining setting, we set number of tokens per batch to be `2^18` tokens. Changing this number might require changes in the lr, lr-scheudler, #steps and #warmup steps. Therefor, it is a good idea to keep this number constant.\n",
    "\n",
    "- Note that: `#tokens/batch = batch_size x #gpus x gradient_accumulation x seqlen`\n",
    "   \n",
    "- In [the paper](https://arxiv.org/pdf/2004.05150.pdf), we train for 65k steps, but 3k is probably enough (check table 6)\n",
    "\n",
    "- **Important note**: The lr-scheduler in [the paper](https://arxiv.org/pdf/2004.05150.pdf) is polynomial_decay with power 3 over 65k steps. To train for 3k steps, use a constant lr-scheduler (after warmup). Both lr-scheduler are not supported in HF trainer, and at least **constant lr-scheduler** will need to be added. \n",
    "\n",
    "- Pretraining will take 2 days on 1 x 32GB GPU with fp32. Consider using fp16 and using more gpus to train faster (if you increase `#gpus`, reduce `gradient_accumulation` to maintain `#tokens/batch` as mentioned earlier).\n",
    "\n",
    "- As a demonstration, this notebook is training on wikitext103 but wikitext103 is rather small that it takes 7 epochs to train for 3k steps Consider doing a single epoch on a larger dataset (800M tokens) instead.\n",
    "\n",
    "- Set #gpus using `CUDA_VISIBLE_DEVICES`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Zl_hDDlryVo2"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelArgs:\n",
    "    attention_window: int = field(default=512, metadata={\"help\": \"Size of attention window\"})\n",
    "    max_pos: int = field(default=8192, metadata={\"help\": \"Maximum position\"})\n",
    "\n",
    "parser = HfArgumentParser((TrainingArguments, ModelArgs,))\n",
    "\n",
    "\n",
    "training_args, model_args = parser.parse_args_into_dataclasses(look_for_args_file=False, args=[\n",
    "    '--output_dir', 'tmp',\n",
    "    '--warmup_steps', '500',\n",
    "    '--learning_rate', '0.00003',\n",
    "    '--weight_decay', '0.01',\n",
    "    '--adam_epsilon', '1e-6',\n",
    "    '--max_steps', '3000',\n",
    "    '--logging_steps', '500',\n",
    "    '--save_steps', '500',\n",
    "    '--max_grad_norm', '5.0',\n",
    "    '--per_gpu_eval_batch_size', '8',\n",
    "    '--per_gpu_train_batch_size', '2',  # 32GB gpu with fp32\n",
    "    '--gradient_accumulation_steps', '32',\n",
    "    '--evaluate_during_training',\n",
    "    '--do_train',\n",
    "    '--do_eval',\n",
    "])\n",
    "\n",
    "# Choose GPU\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MBXU3r69bD6l"
   },
   "source": [
    "1) As descriped in `create_long_model`, convert a `bert-base` model into `bert-base-4096` which is an instance of `BertLong`, then save it to the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7tcsSfZ1-b9",
    "outputId": "38c1941c-7d00-41c2-ef5c-d4a3705e22b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Converting bert-base into bert-base-8192\n",
      "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json not found in cache or force_download set to True, downloading to /home/tomasg/.cache/torch/transformers/tmpmrhwgpq8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb31ebedc5246e685af9349e6a5b5ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json in cache at /home/tomasg/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.file_utils:creating metadata file for /home/tomasg/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-config.json from cache at /home/tomasg/.cache/torch/transformers/4dad0251492946e18ac39290fcfe91b89d370fee250efe9521476438fe8ca185.7156163d5fdc189c3016baca0775ffce230789d7fa2a42ef516483e4ca884517\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.file_utils:https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin not found in cache or force_download set to True, downloading to /home/tomasg/.cache/torch/transformers/tmpgutbfsj9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b52fedc5965463eb80ab0e2e66eac1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:storing https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin in cache at /home/tomasg/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.file_utils:creating metadata file for /home/tomasg/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.modeling_utils:loading weights file https://cdn.huggingface.co/bert-base-uncased-pytorch_model.bin from cache at /home/tomasg/.cache/torch/transformers/f2ee78bdd635b758cc0a12352586868bef80e47401abe4c4fcc3832421e7338b.36ca03ab34a1a5d5fa7bc3d03d55c4fa650fed07220e2eeebc06ce58d0e9a157\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertModel.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of BertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "INFO:transformers.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt not found in cache or force_download set to True, downloading to /home/tomasg/.cache/torch/transformers/tmp8k1nxvy9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "543e3909ef0c476d8f701d61301e86e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.file_utils:storing https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt in cache at /home/tomasg/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.file_utils:creating metadata file for /home/tomasg/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "INFO:transformers.tokenization_utils_base:loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/tomasg/.cache/torch/transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n",
      "2023-03-16 12:14:39.785029: I tensorflow/core/platform/cpu_feature_guard.cc:143] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2023-03-16 12:14:39.834899: I tensorflow/core/platform/profile_utils/cpu_utils.cc:102] CPU Frequency: 2599990000 Hz\n",
      "2023-03-16 12:14:39.835254: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55e3825b4060 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2023-03-16 12:14:39.835270: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
      "2023-03-16 12:14:39.840132: I tensorflow/core/common_runtime/process_util.cc:147] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n",
      "torch.Size([8192, 768])\n",
      "Embedding(512, 768)\n",
      "torch.Size([8192, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:saving model to ./bert-base-8192\n",
      "INFO:transformers.configuration_utils:Configuration saved in ./bert-base-8192/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 8192])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.modeling_utils:Model weights saved in ./bert-base-8192/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "#model_path = f'{training_args.output_dir}/bert-base-{model_args.max_pos}'\n",
    "model_path = f'./bert-base-{model_args.max_pos}'\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    os.makedirs(model_path)\n",
    "\n",
    "logger.info(f'Converting bert-base into bert-base-{model_args.max_pos}')\n",
    "model, tokenizer, new_pos_embed = create_long_model(\n",
    "    save_model_to=model_path, attention_window=model_args.attention_window, max_pos=model_args.max_pos)\n",
    "#create_long_model(save_model_to, attention_window, max_pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JiftMH3-zPUS"
   },
   "source": [
    "2) Load `bert-base-4096` from the disk. This model works for long sequences even without pretraining. If you don't want to pretrain, you can stop here and start finetuning your `bert-base-4096` on downstream tasks ðŸŽ‰ðŸŽ‰ðŸŽ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8vNeYdrzMd2",
    "outputId": "28a5170d-8147-4cc4-a6dd-a73bec482ee5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Loading the model from ./bert-base-8192\n",
      "INFO:transformers.tokenization_utils_base:Model name './bert-base-8192' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, TurkuNLP/bert-base-finnish-cased-v1, TurkuNLP/bert-base-finnish-uncased-v1, wietsedv/bert-base-dutch-cased). Assuming './bert-base-8192' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "INFO:transformers.tokenization_utils_base:Didn't find file ./bert-base-8192/added_tokens.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils_base:Didn't find file ./bert-base-8192/tokenizer.json. We won't load it.\n",
      "INFO:transformers.tokenization_utils_base:loading file ./bert-base-8192/vocab.txt\n",
      "INFO:transformers.tokenization_utils_base:loading file None\n",
      "INFO:transformers.tokenization_utils_base:loading file ./bert-base-8192/special_tokens_map.json\n",
      "INFO:transformers.tokenization_utils_base:loading file ./bert-base-8192/tokenizer_config.json\n",
      "INFO:transformers.tokenization_utils_base:loading file None\n",
      "INFO:transformers.configuration_utils:loading configuration file ./bert-base-8192/config.json\n",
      "INFO:transformers.configuration_utils:Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertModel\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"attention_window\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 8192,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file ./bert-base-8192/pytorch_model.bin\n",
      "INFO:transformers.modeling_utils:All model checkpoint weights were used when initializing BertLong.\n",
      "\n",
      "INFO:transformers.modeling_utils:All the weights of BertLong were initialized from the model checkpoint at ./bert-base-8192.\n",
      "If your task is similar to the task the model of the ckeckpoint was trained on, you can already use BertLong for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Loading the model from {model_path}')\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "model = BertLong.from_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "convert_bert_to_long.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "longbert",
   "language": "python",
   "name": "longbert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
